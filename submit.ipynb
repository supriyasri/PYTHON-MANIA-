{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "submit.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supriyasri/PYTHON-MANIA-/blob/master/submit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns8Rmp--4Nnv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "1f64f93c-7d3b-460b-c6fd-c898dba3ec0f"
      },
      "source": [
        "import os\n",
        "\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "path_data = \"/kaggle/input/ashrae-energy-prediction/\"\n",
        "path_train = path_data + \"train.csv\"\n",
        "path_test = path_data + \"test.csv\"\n",
        "path_building = path_data + \"building_metadata.csv\"\n",
        "path_weather_train = path_data + \"weather_train.csv\"\n",
        "path_weather_test = path_data + \"weather_test.csv\"\n",
        "\n",
        "myfavouritenumber = 13\n",
        "seed = myfavouritenumber"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ef4f643788df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpath_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/ashrae-energy-prediction/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm.notebook'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "466KhDNATg4l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "d6641059-7e2c-445a-b568-9e6164999590"
      },
      "source": [
        "Preparing data\n",
        "There are two files with features that need to be merged with the data. One is building metadata that has information on the buildings and the other is weather data that has information on the weather.\n",
        "\n",
        "df_train = pd.read_csv(path_train)\n",
        "df_test = pd.read_csv(path_test)\n",
        "\n",
        "building = pd.read_csv(path_building)\n",
        "le = LabelEncoder()\n",
        "building.primary_use = le.fit_transform(building.primary_use)\n",
        "\n",
        "weather_train = pd.read_csv(path_weather_train)\n",
        "weather_test = pd.read_csv(path_weather_test)\n",
        "\n",
        "weather_train.drop([\"sea_level_pressure\", \"wind_direction\", \"wind_speed\"], axis=1, inplace=True)\n",
        "weather_test.drop([\"sea_level_pressure\", \"wind_direction\", \"wind_speed\"], axis=1, inplace=True)\n",
        "\n",
        "weather_train = weather_train.groupby(\"site_id\").apply(lambda group: group.interpolate(limit_direction=\"both\"))\n",
        "weather_test = weather_test.groupby(\"site_id\").apply(lambda group: group.interpolate(limit_direction=\"both\"))\n",
        "\n",
        "df_train = df_train.merge(building, on=\"building_id\")\n",
        "df_train = df_train.merge(weather_train, on=[\"site_id\", \"timestamp\"], how=\"left\")\n",
        "df_train = df_train[~((df_train.site_id==0) & (df_train.meter==0) & (df_train.building_id <= 104) & (df_train.timestamp < \"2016-05-21\"))]\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "df_train.timestamp = pd.to_datetime(df_train.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
        "df_train[\"log_meter_reading\"] = np.log1p(df_train.meter_reading)\n",
        "\n",
        "df_test = df_test.merge(building, on=\"building_id\")\n",
        "df_test = df_test.merge(weather_test, on=[\"site_id\", \"timestamp\"], how=\"left\")\n",
        "df_test.reset_index(drop=True, inplace=True)\n",
        "df_test.timestamp = pd.to_datetime(df_test.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "del building, le\n",
        "gc.collect()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b92c80b2a179>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Preparing data\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrJvfZHTTqCy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "2e707492-3e87-49b9-ab56-b509d67beb9c"
      },
      "source": [
        "df_train = reduce_mem_usage(df_train, use_float16=True)\n",
        "df_test = reduce_mem_usage(df_test, use_float16=True)\n",
        "\n",
        "weather_train.timestamp = pd.to_datetime(weather_train.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
        "weather_test.timestamp = pd.to_datetime(weather_test.timestamp, format='%Y-%m-%d %H:%M:%S')\n",
        "weather_train = reduce_mem_usage(weather_train, use_float16=True)\n",
        "weather_test = reduce_mem_usage(weather_test, use_float16=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-140726bce1dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mem_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_float16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mem_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_float16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mweather_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweather_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mweather_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweather_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'reduce_mem_usage' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdRqXs-_TtKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "b9898173-ede2-4b41-a3b2-0863952cbc3c"
      },
      "source": [
        "Feature Engineering: Time\n",
        "Creating time-based features.\n",
        "\n",
        "df_train[\"hour\"] = df_train.timestamp.dt.hour\n",
        "df_train[\"weekday\"] = df_train.timestamp.dt.weekday\n",
        "\n",
        "df_test[\"hour\"] = df_test.timestamp.dt.hour\n",
        "df_test[\"weekday\"] = df_test.timestamp.dt.weekday"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-84139d8d0821>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Feature Engineering: Time\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPEAX4lTTwUg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "78e70029-6920-4d59-cbad-61eaac4efc13"
      },
      "source": [
        "Feature Engineering: Aggregation\n",
        "Creating aggregate features for buildings at various levels.\n",
        "\n",
        "df_building_meter = df_train.groupby([\"building_id\", \"meter\"]).agg(mean_building_meter=(\"log_meter_reading\", \"mean\"),\n",
        "                                                             median_building_meter=(\"log_meter_reading\", \"median\")).reset_index()\n",
        "\n",
        "df_train = df_train.merge(df_building_meter, on=[\"building_id\", \"meter\"])\n",
        "df_test = df_test.merge(df_building_meter, on=[\"building_id\", \"meter\"])\n",
        "\n",
        "df_building_meter_hour = df_train.groupby([\"building_id\", \"meter\", \"hour\"]).agg(mean_building_meter=(\"log_meter_reading\", \"mean\"),\n",
        "                                                                                median_building_meter=(\"log_meter_reading\", \"median\")).reset_index()\n",
        "\n",
        "df_train = df_train.merge(df_building_meter_hour, on=[\"building_id\", \"meter\", \"hour\"])\n",
        "df_test = df_test.merge(df_building_meter_hour, on=[\"building_id\", \"meter\", \"hour\"])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-3c697eeeb116>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Feature Engineering: Aggregation\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRq0F-pVTzUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "742619e8-4d03-43ea-e7ac-30f72cebe4db"
      },
      "source": [
        "Feature Engineering: Lags\n",
        "Creating lag-based features. These are statistics of available features looking back in time by fixed intervals.\n",
        "These features are created in the weather data itself and then merged with the train and test data.\n",
        "\n",
        "def create_lag_features(df, window):\n",
        "    \"\"\"\n",
        "    Creating lag-based features looking back in time.\n",
        "    \"\"\"\n",
        "    \n",
        "    feature_cols = [\"air_temperature\", \"cloud_coverage\", \"dew_temperature\", \"precip_depth_1_hr\"]\n",
        "    df_site = df.groupby(\"site_id\")\n",
        "    \n",
        "    df_rolled = df_site[feature_cols].rolling(window=window, min_periods=0)\n",
        "    \n",
        "    df_mean = df_rolled.mean().reset_index().astype(np.float16)\n",
        "    df_median = df_rolled.median().reset_index().astype(np.float16)\n",
        "    df_min = df_rolled.min().reset_index().astype(np.float16)\n",
        "    df_max = df_rolled.max().reset_index().astype(np.float16)\n",
        "    df_std = df_rolled.std().reset_index().astype(np.float16)\n",
        "    df_skew = df_rolled.skew().reset_index().astype(np.float16)\n",
        "    \n",
        "    for feature in feature_cols:\n",
        "        df[f\"{feature}_mean_lag{window}\"] = df_mean[feature]\n",
        "        df[f\"{feature}_median_lag{window}\"] = df_median[feature]\n",
        "        df[f\"{feature}_min_lag{window}\"] = df_min[feature]\n",
        "        df[f\"{feature}_max_lag{window}\"] = df_max[feature]\n",
        "        df[f\"{feature}_std_lag{window}\"] = df_std[feature]\n",
        "        df[f\"{feature}_skew_lag{window}\"] = df_std[feature]\n",
        "        \n",
        "    return df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-b09dd1130ca3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Feature Engineering: Lags\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fAPUhI3T3VF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "3c2dfd8c-261c-4c93-b608-08e79a8e9d20"
      },
      "source": [
        "Features\n",
        "Creating and selecting all the features.\n",
        "\n",
        "weather_train = create_lag_features(weather_train, 18)\n",
        "weather_train.drop([\"air_temperature\", \"cloud_coverage\", \"dew_temperature\", \"precip_depth_1_hr\"], axis=1, inplace=True)\n",
        "\n",
        "df_train = df_train.merge(weather_train, on=[\"site_id\", \"timestamp\"], how=\"left\")\n",
        "\n",
        "del weather_train\n",
        "gc.collect()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-d214dabe17f8>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Creating and selecting all the features.\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwdJ0IfqT6RT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "a605e189-8e9b-4908-cf37-db3433882c79"
      },
      "source": [
        "categorical_features = [\n",
        "    \"building_id\",\n",
        "    \"primary_use\",\n",
        "    \"meter\",\n",
        "    \"weekday\",\n",
        "    \"hour\"\n",
        "]\n",
        "\n",
        "all_features = [col for col in df_train.columns if col not in [\"timestamp\", \"site_id\", \"meter_reading\", \"log_meter_reading\"]]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7f7a4ba1d601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"site_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"meter_reading\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"log_meter_reading\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxMsc_rCT9Kq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "eccff658-15ba-4f27-9880-cf15ddc0a1f0"
      },
      "source": [
        "KFold Cross Validation with LGBM\n",
        "Since the test data is out of time and longer than train data, creating a reliable validation strategy is going to be a major challenge. Just using a simple KFold CV here.\n",
        "\n",
        "The folds are applied to each site individually, thus building 16 sites x 3 folds = 48 models in total.\n",
        "\n",
        "cv = 2\n",
        "models = {}\n",
        "cv_scores = {\"site_id\": [], \"cv_score\": []}\n",
        "\n",
        "for site_id in tqdm(range(16), desc=\"site_id\"):\n",
        "    print(cv, \"fold CV for site_id:\", site_id)\n",
        "    kf = KFold(n_splits=cv, random_state=seed)\n",
        "    models[site_id] = []\n",
        "\n",
        "    X_train_site = df_train[df_train.site_id==site_id].reset_index(drop=True)\n",
        "    y_train_site = X_train_site.log_meter_reading\n",
        "    y_pred_train_site = np.zeros(X_train_site.shape[0])\n",
        "    \n",
        "    score = 0\n",
        "\n",
        "    for fold, (train_index, valid_index) in enumerate(kf.split(X_train_site, y_train_site)):\n",
        "        X_train, X_valid = X_train_site.loc[train_index, all_features], X_train_site.loc[valid_index, all_features]\n",
        "        y_train, y_valid = y_train_site.iloc[train_index], y_train_site.iloc[valid_index]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "        dvalid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features)\n",
        "\n",
        "        watchlist = [dtrain, dvalid]\n",
        "\n",
        "        params = {\"objective\": \"regression\",\n",
        "                  \"num_leaves\": 41,\n",
        "                  \"learning_rate\": 0.049,\n",
        "                  \"bagging_freq\": 5,\n",
        "                  \"bagging_fraction\": 0.51,\n",
        "                  \"feature_fraction\": 0.81,\n",
        "                  \"metric\": \"rmse\"\n",
        "                  }\n",
        "\n",
        "        model_lgb = lgb.train(params, train_set=dtrain, num_boost_round=999, valid_sets=watchlist, verbose_eval=101, early_stopping_rounds=21)\n",
        "        models[site_id].append(model_lgb)\n",
        "\n",
        "        y_pred_valid = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n",
        "        y_pred_train_site[valid_index] = y_pred_valid\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(y_valid, y_pred_valid))\n",
        "        print(\"Site Id:\", site_id, \", Fold:\", fold+1, \", RMSE:\", rmse)\n",
        "        score += rmse / cv\n",
        "        \n",
        "        gc.collect()\n",
        "        \n",
        "    cv_scores[\"site_id\"].append(site_id)\n",
        "    cv_scores[\"cv_score\"].append(score)\n",
        "        \n",
        "    print(\"\\nSite Id:\", site_id, \", CV RMSE:\", np.sqrt(mean_squared_error(y_train_site, y_pred_train_site)), \"\\n\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-7f7e9e4f435b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    KFold Cross Validation with LGBM\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwL4ftWGUDUv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "bcb18e48-09bb-48c2-e90a-7f4647bf5e58"
      },
      "source": [
        "pd.DataFrame.from_dict(cv_scores)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-957ff92a741e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'cv_scores' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9JwyALnUNdE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "d042ed11-e8c9-4829-b2f5-692fc8ccb282"
      },
      "source": [
        "Scoring on test data\n",
        "The test data for each site is scored individually using the 3 models, one from each fold. The final prediction is the average of the 3 models.\n",
        "\n",
        "weather_test = create_lag_features(weather_test, 18)\n",
        "weather_test.drop([\"air_temperature\", \"cloud_coverage\", \"dew_temperature\", \"precip_depth_1_hr\"], axis=1, inplace=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-876025aa7f82>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Scoring on test data\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrSYZuGeUQcP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "ec553bf3-e1cf-4fd6-fca0-c34cee83f29d"
      },
      "source": [
        "df_test_sites = []\n",
        "\n",
        "for site_id in tqdm(range(16), desc=\"site_id\"):\n",
        "    print(\"Preparing test data for site_id\", site_id)\n",
        "\n",
        "    X_test_site = df_test[df_test.site_id==site_id]\n",
        "    weather_test_site = weather_test[weather_test.site_id==site_id]\n",
        "    \n",
        "    X_test_site = X_test_site.merge(weather_test_site, on=[\"site_id\", \"timestamp\"], how=\"left\")\n",
        "    \n",
        "    row_ids_site = X_test_site.row_id\n",
        "\n",
        "    X_test_site = X_test_site[all_features]\n",
        "    y_pred_test_site = np.zeros(X_test_site.shape[0])\n",
        "\n",
        "    print(\"Scoring for site_id\", site_id)    \n",
        "    for fold in range(cv):\n",
        "        model_lgb = models[site_id][fold]\n",
        "        y_pred_test_site += model_lgb.predict(X_test_site, num_iteration=model_lgb.best_iteration) / cv\n",
        "        gc.collect()\n",
        "        \n",
        "    df_test_site = pd.DataFrame({\"row_id\": row_ids_site, \"meter_reading\": y_pred_test_site})\n",
        "    df_test_sites.append(df_test_site)\n",
        "    \n",
        "    print(\"Scoring for site_id\", site_id, \"completed\\n\")\n",
        "    gc.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7d1a70e0f138>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_test_sites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msite_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"site_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preparing test data for site_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTcQHq4qUVIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "90f47fe7-626b-48f6-acb2-bf70e4410ae7"
      },
      "source": [
        "Submission\n",
        "Preparing final file for submission.\n",
        "\n",
        "submit = pd.concat(df_test_sites)\n",
        "submit.meter_reading = np.clip(np.expm1(submit.meter_reading), 0, a_max=None)\n",
        "submit.to_csv(\"submission_noleak.csv\", index=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-2db1554e282f>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Preparing final file for submission.\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEkHkbIpUZls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "a0f3c5d0-a52c-4e0e-c989-763281302d4d"
      },
      "source": [
        "## adding leak\n",
        "leak0 = pd.read_csv(\"/kaggle/input/ashrae-leak-data/site0.csv\")\n",
        "leak1 = pd.read_csv(\"/kaggle/input/ashrae-leak-data/site1.csv\")\n",
        "leak2 = pd.read_csv(\"/kaggle/input/ashrae-leak-data/site2.csv\")\n",
        "leak4 = pd.read_csv(\"/kaggle/input/ashrae-leak-data/site4.csv\")\n",
        "leak15 = pd.read_csv(\"/kaggle/input/ashrae-leak-data/site15.csv\")\n",
        "\n",
        "leak = pd.concat([leak0, leak1, leak2, leak4, leak15])\n",
        "\n",
        "del leak0, leak1, leak2, leak4, leak15, df_test_sites\n",
        "gc.collect()\n",
        "\n",
        "test = pd.read_csv(path_test)\n",
        "test = test[test.building_id.isin(leak.building_id.unique())]\n",
        "\n",
        "leak = leak.merge(test, on=[\"building_id\", \"meter\", \"timestamp\"])\n",
        "\n",
        "del test\n",
        "gc.collect()\n",
        "\n",
        "submit = submit.merge(leak[[\"row_id\", \"meter_reading_scraped\"]], on=[\"row_id\"], how=\"left\")\n",
        "submit.loc[submit.meter_reading_scraped.notnull(), \"meter_reading\"] = submit.loc[submit.meter_reading_scraped.notnull(), \"meter_reading_scraped\"] \n",
        "submit.drop([\"meter_reading_scraped\"], axis=1, inplace=True)\n",
        "\n",
        "submit.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f21dad2b6446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mleak0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/ashrae-leak-data/site0.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mleak1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/ashrae-leak-data/site1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mleak2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/ashrae-leak-data/site2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mleak4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/ashrae-leak-data/site4.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mleak15\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/ashrae-leak-data/site15.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/kaggle/input/ashrae-leak-data/site0.csv' does not exist: b'/kaggle/input/ashrae-leak-data/site0.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFN3v_Y1Uc_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}